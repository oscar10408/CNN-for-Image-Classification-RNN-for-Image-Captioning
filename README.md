# ğŸ§  CNN & RNN from Scratch in NumPy

This project implements both **Convolutional Neural Networks (CNNs)** for image classification and **Recurrent Neural Networks (RNNs)** for sequence generation, built entirely from scratch using **NumPy**. No external ML libraries like PyTorch or TensorFlow are used. This is an educational codebase to deeply understand the mechanics of deep learning at the matrix level.

---

## ğŸ§© Models Implemented

### ğŸ”¹ 1. CNN for Image Classification

Implemented in: `cnn.py`, `cnn_layers.py`, `layers.py`

**Architecture**:
```
conv â†’ ReLU â†’ maxpool â†’ conv â†’ ReLU â†’ maxpool â†’ fc â†’ ReLU â†’ fc â†’ softmax
```

Supports:
- Custom weight initialization based on fan-in
- Max pooling and convolution implemented via loops
- Softmax classification loss
- Modular layer-wise forward and backward passes

Training loop managed via `Solver` class.

---

### ğŸ”¹ 2. RNN for Image Captioning

Implemented in: `rnn.py`, `rnn_layers.py`, `layers.py`

**Architecture**:
```
Image Feature (via fc) â†’ Initial Hidden State â†’ Word Embedding â†’ RNN â†’ Vocabulary Scores (via fc)
```

Features:
- Implements caption generation with vanilla RNN
- Temporal softmax loss with masking support for padded captions
- Includes forward and backward pass of embedding, RNN, and output projection layers
- Test-time `sample()` method to auto-generate captions from image features

---

## ğŸ› ï¸ Training & Optimization

### ğŸ”§ `solver.py`

A modular training engine inspired by CS231n:
- Supports SGD, Adam (`optim.py`)
- Training/validation accuracy tracking
- Learning rate decay
- Batch sampling and weight updates per epoch

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ cnn.py               # CNN model class
â”œâ”€â”€ cnn_layers.py        # Conv and pooling layers (forward/backward)
â”œâ”€â”€ rnn.py               # Captioning RNN class
â”œâ”€â”€ rnn_layers.py        # RNN step-wise forward/backward logic
â”œâ”€â”€ layers.py            # Shared layers (ReLU, FC, softmax)
â”œâ”€â”€ solver.py            # Training engine for both models
â”œâ”€â”€ optim.py             # Optimizers: SGD, Adam
â”œâ”€â”€ cnn.ipynb            # CNN demo notebook
â”œâ”€â”€ rnn.ipynb            # RNN demo notebook
```

---

## ğŸ§ª Example Usage

### Train a CNN model

```python
from cnn import ConvNet
from solver import Solver

model = ConvNet()
solver = Solver(model, data={
    'X_train': X_train, 'y_train': y_train,
    'X_val': X_val, 'y_val': y_val
}, num_epochs=10, batch_size=64, optim_config={'learning_rate': 1e-3})
solver.train()
```

### Train a Captioning RNN

```python
from rnn import CaptioningRNN
from solver import Solver

model = CaptioningRNN(word_to_idx)
solver = Solver(model, data={
    'X_train': features_train, 'y_train': captions_train,
    'X_val': features_val, 'y_val': captions_val
}, num_epochs=5, batch_size=32, optim_config={'learning_rate': 1e-3})
solver.train()
```

---

## ğŸ¯ Learning Goals

- Understand how forward/backward passes work in CNN and RNN layers
- Implement softmax loss over time series (temporal loss)
- Apply training from scratch without black-box libraries
- Learn how image features can drive sequence generation

---

## ğŸ§± Dependencies

- Python 3.7+
- NumPy

Install:
```bash
pip install numpy
```

---

## ğŸ‘¨â€ğŸ’» Author

**Hao-Chun Shih (Oscar)**  
ğŸ“§ oscar10408@gmail.com  
ğŸ“ University of Michigan â€“ Master in Data Science

---

## ğŸ“œ License

This project is open-source under the [MIT License](https://opensource.org/licenses/MIT).
